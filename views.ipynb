{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938ba394",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d4a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import dotenv\n",
    "import os\n",
    "import asyncio\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from db.db import SongRepository, Song\n",
    "\n",
    "from constants.states import ResponseState\n",
    "from constants.services import Service, service_names_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c7501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb74cb",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DB_URL = os.environ[\"DB_URL\"]\n",
    "\n",
    "YOUTUBE_API_KEYS = os.environ[\"YOUTUBE_API_KEYS\"].split(',')\n",
    "\n",
    "acceptable_sites_map = {\n",
    "    \"Youtube\" : Service.YOUTUBE,\n",
    "    \"NicoNicoDouga\" : Service.NICONICO,\n",
    "    # \"Bilibili\" : Service.BILIBILI\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ff1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SongRepository(DB_URL, False)\n",
    "await db.init_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b9f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_url_id(url: str):\n",
    "    \"\"\"\n",
    "    Examples:\n",
    "    YouTube: https://youtu.be/P9l6Eg_Kk0g\n",
    "    NicoNico: http://www.nicovideo.jp/watch/sm45326679\n",
    "    BiliBili: https://www.bilibili.com/video/av114962936629968\n",
    "    \"\"\"\n",
    "    return url.split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d553a2f",
   "metadata": {},
   "source": [
    "# Websites data fetchers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af02707",
   "metadata": {},
   "source": [
    "## YouTube URL data fetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d579b",
   "metadata": {},
   "source": [
    "Really fast with an API key, and 1 key is enough for 500,000 videos.\n",
    "\n",
    "How to get an API key: https://developers.google.com/youtube/v3/getting-started\n",
    "\n",
    "Without an API key, you will be shadow blocked fairly quickly.\n",
    "\n",
    "Scraper without an API key can be found at ./scrapers/noAPIUnrecommended.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91b4a0",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "YT_BATCH_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55640c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapers.youtubeVideoStatistics import YouTubeScraper\n",
    "\n",
    "yt = YouTubeScraper(api_keys=YOUTUBE_API_KEYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_results_to_updates(url_id_map: dict, res: dict):\n",
    "    updates = []\n",
    "    present_urls = set()\n",
    "    for item in res:\n",
    "        url = f'https://youtu.be/{item[\"id\"]}'\n",
    "        stats = item.get('statistics', {})\n",
    "        present_urls.add(url)\n",
    "        for id_ in url_id_map[url]:\n",
    "            upd = {\n",
    "                'id' : id_,\n",
    "                'url' : url,\n",
    "                'views' : stats.get('viewCount', 0),\n",
    "                'likes' : stats.get('likeCount', None),\n",
    "                'dislikes' : stats.get('dislikeCount', None),\n",
    "                'favorites' : stats.get('favoriteCount', None)\n",
    "            }\n",
    "            updates.append(upd)\n",
    "\n",
    "    for url, ids in url_id_map.items():\n",
    "        if (url not in present_urls):\n",
    "            for id_ in ids:\n",
    "                upd = {\n",
    "                    'id' : id_,\n",
    "                    'url' : url,\n",
    "                    'views' : 0,\n",
    "                    'likes' : 0,\n",
    "                    'dislikes' : 0,\n",
    "                    'favorites' : 0\n",
    "                }\n",
    "                updates.append(upd)\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "while True:\n",
    "    batch = await db.fetch_unprocessed_yt_batch(YT_BATCH_SIZE)\n",
    "    if (len(batch) == 0):\n",
    "        break\n",
    "    cnt += 1\n",
    "    print(batch[0].id)\n",
    "\n",
    "    url_id_map = defaultdict(list)\n",
    "    for row in batch:\n",
    "        url_id_map[row.url].append(row.id)\n",
    "    ids = [parse_url_id(row.url) for row in batch]\n",
    "    res = await yt.fetch_videos_stats(ids)\n",
    "    \n",
    "    updates = yt_results_to_updates(url_id_map, res)\n",
    "\n",
    "    await db.update_song_urls_batch(updates)\n",
    "\n",
    "    print(f\"Finished batch {cnt}\")\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8dcfb5",
   "metadata": {},
   "source": [
    "## NicoNico URL data fetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded510c",
   "metadata": {},
   "source": [
    "Couldn't find an official API, so resulted in using an external one.\n",
    "\n",
    "Couldn't find a documentation for the external API, so I don't know about rate limits, but it seems that the bar is pretty high and you get unblocked fairly quickly (in 1-2 minutes) if you exceed it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cafa7ba",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "NN_BATCH_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc55a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapers.niconicoVideoStatistics import NicoNicoScraper\n",
    "\n",
    "nn = NicoNicoScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_results_to_updates(url_id_map: dict, res: List[Tuple[str, ResponseState, dict]]):\n",
    "    everything_ok = True\n",
    "    updates = []\n",
    "    present_urls = set()\n",
    "    for item in res:\n",
    "        if (item[1] == ResponseState.UNKNOWN):\n",
    "            everything_ok = False\n",
    "            continue\n",
    "\n",
    "        url = f'http://www.nicovideo.jp/watch/{item[0]}'\n",
    "        views = item[2].get('views', 0)\n",
    "        present_urls.add(url)\n",
    "        for id_ in url_id_map[url]:\n",
    "            upd = {\n",
    "                'id' : id_,\n",
    "                'url' : url,\n",
    "                'views' : views\n",
    "            }\n",
    "            updates.append(upd)\n",
    "    \n",
    "    return (everything_ok, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feec120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(\"NicoNicoScraper\")\n",
    "logger.addHandler(ch)\n",
    "\n",
    "cnt = 0\n",
    "while True:\n",
    "    batch = await db.fetch_unprocessed_nn_batch(NN_BATCH_SIZE)\n",
    "    if (len(batch) == 0):\n",
    "        break\n",
    "    cnt += 1\n",
    "    print(batch[0].id)\n",
    "\n",
    "    url_id_map = defaultdict(list)\n",
    "    for row in batch:\n",
    "        url_id_map[row.url].append(row.id)\n",
    "    ids = [parse_url_id(row.url) for row in batch]\n",
    "    res = await nn.get_videos_data(ids)\n",
    "    \n",
    "    state, updates = nn_results_to_updates(url_id_map, res)\n",
    "\n",
    "    await db.update_song_urls_batch(updates)\n",
    "\n",
    "    print(f\"Finished batch {cnt}\")\n",
    "\n",
    "    if (not state):\n",
    "        print(\"Rate limit reached... Sleeping for 120 seconds...\")\n",
    "        await asyncio.sleep(120)\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e5714",
   "metadata": {},
   "source": [
    "## Bilibili URL data fetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0473d6",
   "metadata": {},
   "source": [
    "1. Doesn't like scraping, you have to use user agent.\n",
    "2. Has a really low, undefined rate limit (20 req/sec get rate limited). Therefore, slow af."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9787455",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "BB_BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapers.bilibiliVideoStatistics import BilibiliScraper\n",
    "\n",
    "bb = BilibiliScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5873e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_results_to_updates(url_id_map: dict, res: List[Tuple[str, ResponseState, dict]]):\n",
    "    everything_ok = True\n",
    "    updates = []\n",
    "    present_urls = set()\n",
    "    for item in res:\n",
    "        if (item[1] == ResponseState.UNKNOWN):\n",
    "            everything_ok = False\n",
    "            continue\n",
    "\n",
    "        url = f'https://www.bilibili.com/video/{item[0]}'\n",
    "        present_urls.add(url)\n",
    "        for id_ in url_id_map[url]:\n",
    "            upd = {\n",
    "                'id' : id_,\n",
    "                'url' : url,\n",
    "                'views' : item[2].get('views', 0),\n",
    "                'likes' : item[2].get('likes', 0),\n",
    "                'dislikes' : item[2].get('dislikes', 0),\n",
    "                'favorites' : item[2].get('favorites', 0)\n",
    "            }\n",
    "            updates.append(upd)\n",
    "    \n",
    "    return (everything_ok, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361db86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(\"BilibiliScraper\")\n",
    "logger.addHandler(ch)\n",
    "\n",
    "cnt = 0\n",
    "while True:\n",
    "    batch = await db.fetch_unprocessed_bb_batch(BB_BATCH_SIZE)\n",
    "    if (len(batch) == 0):\n",
    "        break\n",
    "    cnt += 1\n",
    "    print(batch[0].id)\n",
    "\n",
    "    url_id_map = defaultdict(list)\n",
    "    for row in batch:\n",
    "        url_id_map[row.url].append(row.id)\n",
    "    ids = [parse_url_id(row.url) for row in batch]\n",
    "    res = await bb.get_videos_data(ids)\n",
    "    \n",
    "    state, updates = bb_results_to_updates(url_id_map, res)\n",
    "\n",
    "    await db.update_song_urls_batch(updates)\n",
    "\n",
    "    print(f\"Finished batch {cnt}\")\n",
    "\n",
    "    if (not state):\n",
    "        print(\"Rate limit reached... Sleeping for 120 seconds...\")\n",
    "        await asyncio.sleep(120)\n",
    "\n",
    "    await asyncio.sleep(1) # Some delay to not exceed 10 requests per second (20 gets rate limited)\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4e695",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d445860c",
   "metadata": {},
   "source": [
    "TODO:\n",
    "1. SoundClound\n",
    "2. Piapro\n",
    "3. Bandcamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bb8b8",
   "metadata": {},
   "source": [
    "# Merge Song and SongURL tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c64e4a",
   "metadata": {},
   "source": [
    "This section can potentially be really CPU and RAM heavy.\n",
    "\n",
    "Make sure to change these constants according to your system's specifications!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4945e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "MAX_CONCURRENT_BATCHES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fe534",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_order = [\n",
    "    Service.YOUTUBE, Service.NICONICO, Service.BILIBILI\n",
    "]\n",
    "\n",
    "def process_views_data(views: dict) -> dict:\n",
    "    res = {}\n",
    "    for service_name in service_order:\n",
    "        service = service_names_map[service_name]\n",
    "        if (service in views.keys()):\n",
    "            res[service] = views[service]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d72575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "sem = asyncio.Semaphore(MAX_CONCURRENT_BATCHES)\n",
    "\n",
    "async def process_batch(batch, cnt):\n",
    "    async with sem:\n",
    "        updates = [\n",
    "            {\n",
    "                \"id\": song[\"song_id\"],\n",
    "                \"views\": {\n",
    "                    service_names_map[s]: song[\"services\"][service_names_map[s]]\n",
    "                    for s in service_order\n",
    "                    if service_names_map[s] in song[\"services\"]\n",
    "                },\n",
    "            }\n",
    "            for song in batch\n",
    "        ]\n",
    "        await db.update_songs_batch(updates)\n",
    "        print(f\"Updated batch {cnt}\")\n",
    "\n",
    "async def run_all_batches():\n",
    "    pending = set()\n",
    "    cnt = 0\n",
    "\n",
    "    async for batch in db.fetch_joined_views_in_batches(BATCH_SIZE):\n",
    "        cnt += 1\n",
    "        task = asyncio.create_task(process_batch(batch, cnt))\n",
    "        pending.add(task)\n",
    "\n",
    "        if len(pending) >= MAX_CONCURRENT_BATCHES * 2:\n",
    "            done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n",
    "\n",
    "    await asyncio.gather(*pending)\n",
    "    print(\"Finished updating DB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fad477",
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_all_batches()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
